---
title: "Assignment 3: Advanced Machine Learning with YouTube Trending Data"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set CRAN mirror explicitly for knitting / non-interactive runs
options(repos = c(CRAN = "https://cloud.r-project.org"))

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(dplyr)
library(stringr)
library(glmnet)
library(ranger)
library(xgboost)
library(tidyverse)
library(rpart)
library(caret)
```

## Problem Overview

The goal is learning more advanced ML techniques including regularized regression and ensemble methods such as bootstraping,bagging and boosting.

## Introduction

YouTube is the worldâ€™s largest video-sharing platform. Videos can "trend" in one or more countries, and their popularity is influenced by many factors such as category, duration, channel, and engagement metrics.


## Data

There are more than 100,000 YouTube trending videos in the dataset. Each row contains metadata about a trending video, including its category, country, channel, engagement metrics, and more.

You are provided with the following files:

1. `yt_training_X.csv`: features for the training instances (videos published before a certain date).
2. `yt_training_y.csv`: labels for the target variable for the training instances.
3. `yt_test_X.csv`: features for the test instances (videos published after that date). Your goal is to make predictions for the instances in the test set.

For convenience, a smaller subset of the training data is also provided:

4. `yt_small_training_X.csv`
5. `yt_small_training_y.csv`

A "fake" test set of y-variable labels (all "NO") is provided for coding convenience:

6. `yt_test_y_FAKE.csv`

---

## Assignment:-

## Setup part 1: 
```{r setup1}
library(readr)
library(dplyr)
library(stringr)
library(caret)

train_X <- read_csv("yt_small_training_X.csv", show_col_types = FALSE)
train_y <- read_csv("yt_small_training_y.csv", show_col_types = FALSE)
#train_X <- read_csv("yt_training_X.csv", show_col_types = FALSE)
#train_y <- read_csv("yt_training_y.csv", show_col_types = FALSE)
test_X  <- read_csv("yt_test_X.csv",     show_col_types = FALSE)
test_y  <- read_csv("yt_test_y_FAKE.csv", show_col_types = FALSE)

train <- train_X %>%
  mutate(success = as.factor(train_y$success),
         original_set = "tr")

test <- test_X %>%
  mutate(success = as.factor(test_y$success),
         original_set = "te")


all_data <- rbind(train, test)


subset <- all_data %>%
  select(
    video_view_count, video_like_count, video_comment_count, video_duration_seconds,
    video_category_id, channel_country, video_definition, channel_subscriber_count,
    success,             
    video_tags,
    original_set          
  ) %>%
  mutate(
    video_category_id = as.factor(video_category_id),
    channel_country   = as.factor(channel_country),
    video_definition  = as.factor(video_definition),
    success           = as.factor(success),
    likes_per_view    = ifelse(video_view_count > 0, video_like_count / video_view_count, 0),
    comments_per_view = ifelse(video_view_count > 0, video_comment_count / video_view_count, 0),
    is_hd             = ifelse(tolower(as.character(video_definition)) == "hd", 1, 0),
  ) %>%
  group_by(video_category_id) %>%
  mutate(mean_views = mean(video_view_count, na.rm = TRUE),
         high_for_category = ifelse(video_view_count > mean_views, "YES", "NO")) %>%
  ungroup() %>%
  select(-mean_views) %>%
  mutate(high_for_category = as.factor(high_for_category)) %>%
  select(
    video_view_count, video_like_count, video_comment_count, video_duration_seconds,
    video_category_id, channel_country, video_definition, channel_subscriber_count,
    likes_per_view, comments_per_view, is_hd, high_for_category,
    success, original_set
  ) %>%
  mutate(video_category_id = ifelse(is.na(video_category_id), 'Entertainment', video_category_id),
         channel_country = as.factor(ifelse(is.na(channel_country), 'unknown', channel_country)))


# --- one-hot 
dummy   <- dummyVars(~ . - success, data = subset, fullRank = TRUE)
one_hot <- predict(dummy, newdata = subset)
dim(one_hot)

os_idx <- ncol(one_hot)


test_inds        <- one_hot[, os_idx] == 0
outer_train_inds <- one_hot[, os_idx] == 1
x_test        <- one_hot[test_inds,        1:(os_idx - 1), drop = FALSE]
x_outer_train <- one_hot[outer_train_inds, 1:(os_idx - 1), drop = FALSE]

y_test        <- test_y  %>% transmute(success = as.factor(success))
y_outer_train <- train_y %>% transmute(success = as.factor(success))

set.seed(123)
inner_train_inds <- sample(nrow(x_outer_train), size = floor(0.7 * nrow(x_outer_train)))

x_train <- x_outer_train[inner_train_inds, , drop = FALSE]
y_train <- y_outer_train[inner_train_inds, , drop = FALSE]

x_valid <- x_outer_train[-inner_train_inds, , drop = FALSE]
y_valid <- y_outer_train[-inner_train_inds, , drop = FALSE]

y_train <- y_train$success
y_valid <- y_valid$success


```

---

## Setup part 2: 

**Some useful functions**
```{r setup2}
# classify:
# input is probabilities and a cutoff
# output is 1/0 values 
classify <- function(probabilities, cutoff){
  classifications <- ifelse(probabilities >= cutoff, "YES", "NO")
  return(classifications)
}

# accuracy: 
# input is actual values and classifications
# output is accuracy
accuracy <- function(actuals, classifications){
  
  #check if the actuals and predictions match
  #note - they have to be in EXACTLY the same format
  is_correct <- ifelse(actuals == classifications, 1, 0)
  
  #accuracy is the average # of correct classifications
  acc <- mean(is_correct)
  return(acc)
}

# finds the best lambda value and prints performance
# input is a vector of accuracy values corresponding to a grid value
# returns the best lambda and prints the accuracy with that lambda
get_best_lambda <- function(accs, grid_values){
  best_index <- which.max(accs)
  best_lambda <- grid_values[best_index]
  best_acc <- accs[best_index]
  
  print(paste('The highest accuracy was ', best_acc, ' which occured when lambda = ',best_lambda,'.',sep = ''))
  return(best_lambda)
}

```

---

## 1: Ridge and Lasso

a. Using a grid of 100 lambda values ranging between 10^-2 and 10^2, find the optimal lambda for a ridge-penalized logistic regression model based on the validation accuracy (you can assume a cutoff of 0.5 for all problems in this assignment). Plot the fitting curve and report the optimal lambda. 

**ANSWER TO QUESTION 1a HERE:** 
I have used a grid of 100 lambda values ranging between 10^-2 and 10^2. The optimal lambda value is 0.01 and the highest accuracy is 0.7153333.
```{r code1a}
library(glmnet)

# Define grid of lambda values
lambda_grid <- exp(seq(log(1e-2), log(1e2), length.out = 100))

# Fit ridge logistic regression for each lambda
ridge_accs <- c()

for (l in lambda_grid) {
  fit <- glmnet(x = x_train, y = y_train, 
                family = "binomial", 
                alpha = 0,       # alpha=0 => ridge
                lambda = l)
  
  preds_prob <- predict(fit, newx = x_valid, type = "response")
  preds_class <- classify(preds_prob, 0.5)
  
  ridge_accs <- c(ridge_accs, accuracy(y_valid, preds_class))
}

# Find best lambda
best_lambda_ridge <- get_best_lambda(ridge_accs, lambda_grid)

# Plot accuracy vs log(lambda)
plot(log(lambda_grid), ridge_accs, type = "l", lwd = 2,
     xlab = "log(lambda)", ylab = "Validation Accuracy",
     main = "Ridge Logistic Regression: Validation Accuracy Curve")
abline(v = log(best_lambda_ridge), col = "red", lty = 2)

```

b. Repeat part (a) for a lasso-penalized logistic regression model.

**ANSWER TO QUESTION 1b HERE:** 
Again the best lambda value is 0.01. But the highest accuracy is 0.699667.
```{r code1b}
# Define grid of lambda values
lambda_grid <- exp(seq(log(1e-2), log(1e2), length.out = 100))

# Fit lasso logistic regression for each lambda
lasso_accs <- c()

for (l in lambda_grid) {
  fit <- glmnet(x = x_train, y = y_train, 
                family = "binomial", 
                alpha = 1,       # alpha=1 => lasso
                lambda = l)
  
  preds_prob <- predict(fit, newx = x_valid, type = "response")
  preds_class <- classify(preds_prob, 0.5)
  
  lasso_accs <- c(lasso_accs, accuracy(y_valid, preds_class))
}

# Find best lambda
best_lambda_lasso <- get_best_lambda(lasso_accs, lambda_grid)

# Plot accuracy vs log(lambda)
plot(log(lambda_grid), lasso_accs, type = "l", lwd = 2,
     xlab = "log(lambda)", ylab = "Validation Accuracy",
     main = "Lasso Logistic Regression: Validation Accuracy Curve")
abline(v = log(best_lambda_lasso), col = "red", lty = 2)

```

c. Do the ridge and lasso models have the same best lambda? Do the best ridge and best lasso models have the same coefficients? Which one has better validation accuracy?

**ANSWER TO QUESTION 1c HERE:** 
1) YES. Both ridge and lasso models have same best lambda and the value is 0.01.
2) No. Both have different coefficients. It is evident by the different number of non-zero coefficients in RIDGE(75) and LASSO(48).
3) RIDGE model (0.715333) has a higher validation accuracy than LASSO (0.699667).
```{r code1c}
### c. Comparing Ridge and Lasso models

# Compare best lambdas
cat("Best Ridge Lambda:", best_lambda_ridge, "\n")
cat("Best Lasso Lambda:", best_lambda_lasso, "\n\n")

# Get coefficients at best lambda
ridge_fit <- glmnet(x = x_train, y = y_train, 
                    family = "binomial", 
                    alpha = 0, 
                    lambda = best_lambda_ridge)
lasso_fit <- glmnet(x = x_train, y = y_train, 
                    family = "binomial", 
                    alpha = 1, 
                    lambda = best_lambda_lasso)

ridge_coefs <- coef(ridge_fit)
lasso_coefs <- coef(lasso_fit)

cat("Non-zero Ridge coefficients:", sum(ridge_coefs != 0), "\n")
cat("Non-zero Lasso coefficients:", sum(lasso_coefs != 0), "\n\n")

# Compare accuracies
cat("Ridge Accuracy:", max(ridge_accs), "\n")
cat("Lasso Accuracy:", max(lasso_accs), "\n")
```

---

## 2: Ensemble Methods

a. Use ranger() to train a random forest model with "default" hyperparameters: 500 trees and m = 10. Estimate predictions in the validation set and report the accuracy. 

**ANSWER TO QUESTION 2a HERE:** 
Validation accuracy of this model is 0.751333.
```{r code2a}
library(ranger)

# Train random forest with 500 trees and m = 10
rf_model <- ranger(
  formula = y_train ~ .,
  data = data.frame(x_train, y_train),
  num.trees = 500,
  mtry = 10,
  probability = TRUE
)

# Predict on validation set
rf_preds_prob <- predict(rf_model, data.frame(x_valid))$predictions[, "YES"]

# Convert to classifications with cutoff 0.5
rf_preds_class <- classify(rf_preds_prob, 0.5)

# Compute accuracy
rf_acc <- accuracy(y_valid, rf_preds_class)
rf_acc

```

b. Create a variable importance plot. Which are the top 5 most important variables?

**ANSWER TO QUESTION 2b HERE:** 
The 5 most important variables are:- video_view_count,channel_subscriber_count,  video_like_count,video_duration_seconds and video_comment_count. 
```{r code2b}
# Train RF again but include importance = "impurity"
rf_model_imp <- ranger(
  formula = y_train ~ .,
  data = data.frame(x_train, y_train),
  num.trees = 500,
  mtry = 10,
  importance = "impurity",
  probability = TRUE
)

# Extract variable importance
var_imp <- rf_model_imp$variable.importance

# Sort by importance
var_imp_sorted <- sort(var_imp, decreasing = TRUE)

# Plot variable importance
barplot(var_imp_sorted[1:20],
        las = 2, cex.names = 0.7,
        main = "Top 20 Variable Importances (Random Forest)",
        ylab = "Importance")

# Print top 5 variables
head(var_imp_sorted, 5)

```

c. Use xgboost() to train a boosting model with default parameters (max.depth = 6, eta = .3, and nrounds = 500). Estimate predictions in the validation set and report the accuracy. Create another variable importance plot. Are the top 5 most important features the same as for the random forest model?

**ANSWER TO QUESTION 2c HERE:** 
The validation accuracy of the XGBoost model is 0.721.

The top 5 most important features are the same as those for the random forest model, but their relative ordering is different. This difference arises because random forest measures importance by Gini impurity reduction, while XGBoost measures importance by Gain (the reduction in loss).
```{r code2c}
library(xgboost)

# Convert training data to xgb.DMatrix
dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = as.numeric(y_train) - 1) # YES=1, NO=0
dvalid <- xgb.DMatrix(data = as.matrix(x_valid), label = as.numeric(y_valid) - 1)

# Train XGBoost model
xgb_model <- xgboost(
  data = dtrain,
  objective = "binary:logistic",
  max.depth = 6,
  eta = 0.3,
  nrounds = 500,
  verbose = 0
)

# Predict on validation set
xgb_preds_prob <- predict(xgb_model, dvalid)
xgb_preds_class <- classify(xgb_preds_prob, 0.5)

# Compute accuracy
xgb_acc <- accuracy(y_valid, xgb_preds_class)
xgb_acc

# Variable importance
xgb_imp <- xgb.importance(model = xgb_model)
xgb.plot.importance(xgb_imp[1:20,], main = "Top 20 Feature Importances (XGBoost)")

# Print top 5 features
xgb_imp[1:5, ]

```

d. Conduct a grid search that loops over at least three hyperparameters and at least two values for each hyperparameter, for either random forest or boosting. Find the optimal hyperparameters from your loop. By how much does the validation accuracy improve versus the default version that you ran in (a) or (c)?

**ANSWER TO QUESTION 2d HERE:** 
I have conducted a grid search for BOOOSTING using the following hyperparameters and their values:- "max.depth" with values (2,4,6);"eta" with value (0.1,0.3) and "nrounds" with values (200,500).

Validation accuracy in case of the systematic grid-search is 0.739.
Compared to (a), validation accuracy REDUCES by 0.01233(1.233%).
Compared to (c), validation accuracy IMPROVES by 0.018(1.80%)
```{r code2d}
# Define hyperparameter grid
grid <- expand.grid(
  max.depth = c(4, 6, 8),
  eta       = c(0.1, 0.3),
  nrounds   = c(200, 500)
)

# Data matrices
dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = as.numeric(y_train) - 1)
dvalid <- xgb.DMatrix(data = as.matrix(x_valid), label = as.numeric(y_valid) - 1)

results <- data.frame(max.depth = integer(), eta = numeric(), nrounds = integer(), acc = numeric())

# Loop through grid
for (i in 1:nrow(grid)) {
  params <- list(
    objective = "binary:logistic",
    max.depth = grid$max.depth[i],
    eta = grid$eta[i]
  )
  
  model <- xgboost(
    params = params,
    data = dtrain,
    nrounds = grid$nrounds[i],
    verbose = 0
  )
  
  preds_prob <- predict(model, dvalid)
  preds_class <- classify(preds_prob, 0.5)
  acc <- accuracy(y_valid, preds_class)
  
  results <- rbind(results, data.frame(
    max.depth = grid$max.depth[i],
    eta = grid$eta[i],
    nrounds = grid$nrounds[i],
    acc = acc
  ))
}

# Find best hyperparameters
best_result <- results[which.max(results$acc), ]
best_result

```

e. Now do a randomized grid search to optimize the hyperparameter selection for the same model as in part (d), using the same number of iterations as your original grid search. Evaluate the best model in the validation data. Which version performs better in the validation set - systematic or randomized grid search?

**ANSWER TO QUESTION 2e HERE:** 
Validation accuracy in case of the randomized grid search is 0.739, which is the same as the accuracy of systematic grid-search in(d). Hence, both versions perform equally well.
```{r code2e}
set.seed(123) # for reproducibility

# Define candidate values for each hyperparameter
max_depth_vals <- c(4, 6, 8)
eta_vals <- c(0.1, 0.3)
nrounds_vals <- c(200, 500)

# Number of iterations = same as full grid search (12)
n_iter <- 12

results_rand <- data.frame(max.depth = integer(), eta = numeric(), nrounds = integer(), acc = numeric())

for (i in 1:n_iter) {
  # Randomly sample one value for each hyperparameter
  md <- sample(max_depth_vals, 1)
  et <- sample(eta_vals, 1)
  nr <- sample(nrounds_vals, 1)
  
  params <- list(
    objective = "binary:logistic",
    max.depth = md,
    eta = et
  )
  
  model <- xgboost(
    params = params,
    data = dtrain,
    nrounds = nr,
    verbose = 0
  )
  
  preds_prob <- predict(model, dvalid)
  preds_class <- classify(preds_prob, 0.5)
  acc <- accuracy(y_valid, preds_class)
  
  results_rand <- rbind(results_rand, data.frame(
    max.depth = md,
    eta = et,
    nrounds = nr,
    acc = acc
  ))
}

# Best randomized result
best_result_rand <- results_rand[which.max(results_rand$acc), ]
best_result_rand

```

f. Based on this 70/30 training/validation split, which is the best model that you tried so far on this assignment? Is this better than the best performance you found in the first two assignments?

**ANSWER TO QUESTION 2f HERE:** 
In this assignment, the best model in this assignment is the RANDOM FOREST model trained in 2(a) as it has the highest validation accuracy @ 0.75133 (75.133%).
YES. this model is far better the best models in assignment1(decision tree with maxdepth=8 with ACCURACY of 0.62) and in assignment2(decision tree with maxdepth=7 with ACCURACY of 0.6691).

## 3: OPTIONAL ENHANCEMENTS - can be done in teams

Here are some more things you can try to improve the performance of your model (see HW1 and HW2 for more suggestions).

- Conduct a more involved grid search over more hyperparameters with more possible values. This could be parallelized across multiple group members.
- Use cross-validation for model selection.
- Experiment with larger training set sizes - see if the optimal models/hyperparameters are the same. 
- As always, create more and more features!

Include the code you used to generate your enhanced predictions in the code block below.

```{r code3}
# ============================================
# FAST & STRONG FINAL MODEL
# (Random Forest + XGBoost ensemble, tuned light)
# ============================================

set.seed(123)
install.packages("doParallel")
library(caret)
library(ranger)
library(xgboost)
library(dplyr)
library(doParallel)

# --- Parallel setup ---
cl <- makeCluster(parallel::detectCores() - 1)
registerDoParallel(cl)

# --- Factors ---
y_train_fac <- factor(y_train, levels = c("NO","YES"))
y_valid_fac <- factor(y_valid, levels = c("NO","YES"))
y_outer_fac <- factor(y_outer_train$success, levels = c("NO","YES"))

# --- CV control ---
ctrl_cv <- trainControl(method = "cv", number = 3,
                        classProbs = TRUE, savePredictions = "final")

# --- Random Forest (light grid) ---
rf_grid <- expand.grid(
  mtry = c(8, 12),
  splitrule = "gini",
  min.node.size = c(5)
)

rf_cv <- train(
  x = x_train, y = y_train_fac,
  method = "ranger",
  trControl = ctrl_cv,
  tuneGrid = rf_grid,
  metric = "Accuracy",
  num.trees = 500,
  importance = "impurity"
)

best_rf_mtry <- rf_cv$bestTune$mtry
best_rf_min  <- rf_cv$bestTune$min.node.size
rf_valid_prob <- predict(rf_cv$finalModel,
                         data.frame(x_valid))$predictions[, "YES"]

# --- XGBoost (light grid) ---
xgb_grid <- expand.grid(
  nrounds = c(400, 600),
  max_depth = c(4, 6),
  eta = c(0.05, 0.1),
  gamma = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample = 0.8
)

xgb_cv <- train(
  x = x_train, y = y_train_fac,
  method = "xgbTree",
  trControl = ctrl_cv,
  tuneGrid = xgb_grid,
  metric = "Accuracy",
  verbose = FALSE
)

xgb_valid_prob <- predict(xgb_cv, newdata = x_valid,
                          type = "prob")[, "YES"]

# --- Ensemble tuning (light search) ---
classify_vec <- function(prob, cutoff = 0.5)
  ifelse(prob >= cutoff, "YES", "NO")
acc_fn <- function(act, pred) mean(act == pred)

weights <- seq(0.3, 0.7, by = 0.1)
cutoffs <- seq(0.4, 0.6, by = 0.02)

best_val_acc <- -Inf
best_w <- 0.5
best_cut <- 0.5

for (w in weights) {
  ens_prob <- w * rf_valid_prob + (1 - w) * xgb_valid_prob
  for (c0 in cutoffs) {
    pred <- classify_vec(ens_prob, c0)
    acc  <- acc_fn(y_valid_fac, pred)
    if (acc > best_val_acc) {
      best_val_acc <- acc; best_w <- w; best_cut <- c0
    }
  }
}

cat("Best ensemble: RF weight =", best_w,
    "cutoff =", best_cut, "val_acc =", round(best_val_acc, 4), "\n")

# --- Retrain full models ---
rf_final <- ranger(
  formula = y_outer_fac ~ .,
  data = data.frame(x_outer_train, y_outer_fac),
  num.trees = 500,
  mtry = best_rf_mtry,
  min.node.size = best_rf_min,
  probability = TRUE
)

dtrain_full <- xgb.DMatrix(as.matrix(x_outer_train),
                           label = as.numeric(y_outer_fac) - 1)
xgb_best <- xgb.train(
  params = list(objective = "binary:logistic",
                max_depth = xgb_cv$bestTune$max_depth,
                eta = xgb_cv$bestTune$eta,
                gamma = xgb_cv$bestTune$gamma,
                colsample_bytree = xgb_cv$bestTune$colsample_bytree,
                min_child_weight = xgb_cv$bestTune$min_child_weight,
                subsample = xgb_cv$bestTune$subsample,
                eval_metric = "logloss"),
  data = dtrain_full,
  nrounds = xgb_cv$bestTune$nrounds,
  verbose = 0
)

# --- Predict on test ---
rf_test_prob  <- predict(rf_final, data.frame(x_test))$predictions[, "YES"]
xgb_test_prob <- predict(xgb_best, xgb.DMatrix(as.matrix(x_test)))
final_prob <- best_w * rf_test_prob + (1 - best_w) * xgb_test_prob
final_pred <- ifelse(final_prob >= best_cut, "YES", "NO")

submission <- data.frame(success = final_pred)
write.csv(submission, "success_group03_finalpred.csv", row.names = FALSE)
print(table(submission$success))

# --- Clean up ---
stopCluster(cl)

```